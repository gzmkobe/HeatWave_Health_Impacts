---
title: "Untitled"
author: "Zhiming Guo"
date: "May 23, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load required libraries

```{r message = FALSE, warning = FALSE}
# Data wrangling
library(lubridate)
library(dplyr)

# For plotting
library(ggplot2)

# For regression tree
# library(MASS)
library(tree)

# For bagging and random forest models
library(randomForest)

### For the boosting model
library(gbm)


### For tuning models
library(caret)


### Fo xgboost model
library(xgboost)
```


Load and clean up data:

```{r}
load("data-raw/ListOfAllHeatwaves.Rdata")
fix_colnames <- which(colnames(all.hws) == "mean.temp")
colnames(all.hws)[fix_colnames[2]] <- "mean.temp.1"

hw <- all.hws %>% 
  mutate(start.year = year(start.date))
colnames(hw) <- gsub(" ", ".", colnames(hw))
```

Create dataframes for fitting models for y2 and y2:

```{r}
hw <- hw %>%
  select(-hw.number, -start.date, -end.date, -id, -Std..Error, 
         -city, -Posterior.Estimate, -Posterior.SE, -post.se)


to_fit_y2 <- hw %>%
  select(-Estimate)

# Create a random subset of heat waves to use to train models
set.seed(1001) ##generate fixed random numbers
hw_indices <- 1:nrow(hw)

train <- sort(sample(hw_indices, length(hw_indices) / 2))
head(train)

test <- hw_indices[-train]
head(test)
```

Fit a regression tree for $Y_1$:

```{r}
tree.hw <- tree(post.estimate ~ ., data = to_fit_y2, subset = train)
summary(tree.hw)
```

The variables included in this model are: `r paste(summary(tree.hw)$used, collapse = ", ")`. Here is a plot of the final tree for this model:

```{r}
plot(tree.hw)
text(tree.hw, pretty = 0)
```

Here are the results of using cross-validation to decide whether (and how much) to prune the tree:

```{r}
cv.hw <- cv.tree(tree.hw)
plot(cv.hw$size, cv.hw$dev, type='b')
```

Based on this analysis, the best tree size would be `r cv.hw$size[4]` nodes.

Using the unpruned tree, here are predictions on the holdout, training dataset:

```{r fig.width = 6, fig.height = 5}
yhat <- predict(tree.hw, newdata = to_fit_y2[test, ])

to_plot <- data.frame(post.estimated_y2 = yhat,
                      true_y2 = to_fit_y2[test, "post.estimate"])
ggplot(to_plot, aes(x = post.estimated_y2, y = true_y2)) + 
  geom_point(alpha = 0.2) + 
  xlim(range(to_plot)) + ylim(range(to_plot)) + 
  theme_minimal() + 
  geom_abline(intercept = 0, slope = 1) + 
  xlab("post.estimated Y_1") + ylab("True Y_1")
```

The mean squared error for this model was `r mean((to_plot$post.estimated_y2 - to_plot$true_y2)^2)`.

Fit a bagging model:

```{r fig.width = 6, fig.height = 5}
set.seed(1)
bag.hw <- randomForest(post.estimate ~ ., 
                       data = to_fit_y2, subset = train,
                       mtry = ncol(to_fit_y2) - 1, importance = TRUE)

yhat.bag <- predict(bag.hw, newdata = to_fit_y2[test, ])

to_plot <- data.frame(post.estimated_y2 = yhat.bag,
                      true_y2 = to_fit_y2[test, "post.estimate"])
ggplot(to_plot, aes(x = post.estimated_y2, y = true_y2)) + 
  geom_point(alpha = 0.2) + 
  xlim(range(to_plot)) + ylim(range(to_plot)) + 
  theme_minimal() + 
  geom_abline(intercept = 0, slope = 1) + 
  xlab("post.estimated Y_1") + ylab("True Y_1")
```

The mean squared error for this model was `r mean(to_plot$post.estimated_y2 - to_plot$true_y2)^2`.

Here is more on the variable importance for this model:

```{r}
importance(bag.hw)
varImpPlot(bag.hw)
```

Fitting a random forest model, with tuning:

```{r}
# Use 10-fold cross validation for tuning to find the best `mtry`
fitControl <- trainControl(method = "cv", number = 10)

set.seed(1)
tuning.rf.hw <- train(post.estimate ~ ., data = to_fit_y2, subset = train,
                   method = "rf", trControl = fitControl, ntree = 10,
                   importance = TRUE, metric="RMSE",
                   maximize = FALSE, tuneLength=5)
```

Here are the results from that tuning process:

```{r}
tuning.rf.hw
plot(tuning.rf.hw)
```

Based on this tuning, the best value for `mtry` in the random forest model for this outcome is `r tuning.rf.hw$bestTune`. 

```{r fig.width = 6, fig.height = 5}
yhat.rf <- predict(tuning.rf.hw$finalModel, newdata = to_fit_y2[test, ])

to_plot <- data.frame(post.estimated_y2 = yhat.rf,
                      true_y2 = to_fit_y2[test, "post.estimate"])
ggplot(to_plot, aes(x = post.estimated_y2, y = true_y2)) + 
  geom_point(alpha = 0.2) + 
  xlim(range(to_plot)) + ylim(range(to_plot)) + 
  theme_minimal() + 
  geom_abline(intercept = 0, slope = 1) + 
  xlab("post.estimated Y_1") + ylab("True Y_1")
```

The mean squared error for this model was `r mean(to_plot$post.estimated_y2 - to_plot$true_y2)^2`.

Here are the variable importance plots: 

```{r fig.width = 8, fig.height = 8}
importance(tuning.rf.hw$finalModel)
varImpPlot(tuning.rf.hw$finalModel)
```

Fit a boosting model, with tuning

```{r}
#### Tuning to find best mtry for Boosting
gbmgrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
                    .n.trees = seq(200, 300, by = 50),
                    .shrinkage = 0.01,
                    .n.minobsinnode = 10)
set.seed(1)


tuning.boost.hw <- train(post.estimate~., data=to_fit_y2, subset = train,
                      method = "gbm",
                      tuneGrid = gbmgrid,
                      trControl = fitControl,
                      verbose = FALSE)
```

Here are the results from that tuning process:

```{r}
tuning.boost.hw
plot(tuning.boost.hw)
```

Based on this tuning, the best value for `interaction.depth` is `rtuning.boost.hw$bestTune[,2]` and for `n.trees` is `r tuning.boost.hw$bestTune[,1]`. 

```{r}
boost.hw <- gbm(post.estimate~., data = to_fit_y2[train,],
                distribution = "gaussian",
                n.trees = tuning.boost.hw$bestTune[,1],
                interaction.depth = tuning.boost.hw$bestTune[,2])
summary(boost.hw)

yhat.boost <- predict(boost.hw, newdata = to_fit_y2[test,], n.trees =tuning.boost.hw$bestTune[,1])
hw.test <- to_fit_y2[-train, "post.estimate"]


to_plot <- data.frame(post.estimated_y2 = yhat.boost,
                      true_y2 = to_fit_y2[test, "post.estimate"])
ggplot(to_plot, aes(x = post.estimated_y2, y = true_y2)) + 
  geom_point(alpha = 0.2) + 
  xlim(range(to_plot)) + ylim(range(to_plot)) + 
  theme_minimal() + 
  geom_abline(intercept = 0, slope = 1) + 
  xlab("post.estimated Y_1") + ylab("True Y_1")

```

The mean squared error for this model was `r mean(to_plot$post.estimated_y2 - to_plot$true_y2)^2`.



Fit a xgboost model.
```{r}
df_train <- to_fit_y2[train,]

df_test <- to_fit_y2[-train,]

data <- as.matrix(df_train[,-25])
label=df_train[,25]
```

Tuning
```{r}
xgb.grid <- expand.grid(nrounds = 10,
                        eta = c(0.01,0.05,0.1),  
                        max_depth = c(2,4,6,8,10,14),
                        gamma = 1,
                        colsample_bytree = 0.5,
                        min_child_weight=1
                        )
set.seed(45)
xgb_tune <-train(post.estimate~.,
                 data=df_train,
                 method="xgbTree",
                 trControl=fitControl,
                 tuneGrid=xgb.grid,
                 verbose=T,
                 nthread =3)
xgb_tune
```

Based on this tuning, the best value for `max_depth` is `r xgb_tune$bestTune[,1]` and for `shrinkage` is `r xgb_tune$bestTune[,2]`. 







