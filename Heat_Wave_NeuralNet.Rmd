---
title: "Neural Network"
author: "Zhiming Guo"
date: "June 6, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}

##load data and prepare to fit the nerualnet
load("data-raw/ListOfAllHeatwaves.Rdata")


library(lubridate)   ## consider start.date as a predictive variable but we need to change its type into numeric as year
hw=data.frame(all.hws)
colnames(hw)[28]<="mean.temp.1"   
hw$start.date=year(all.hws$start.date)
hw=hw[,-c(1,7,20,22,31:35)]   ## delete useless columns

library(MASS)         
set.seed(1)
data=hw
apply(data,2,function(x) sum(is.na(x)))    ## check if missing data exists

index <- sample(1: nrow (hw), nrow(hw)/2)  ## split data into train and test set 
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(Estimate~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$Estimate)^2)/nrow(test)


## using the min-max method and scale the data in the interval [0,1]
maxs <- apply(data, 2, max) 
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
```

```{r}  
##### Form neuralnet based on trinning data set
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("Estimate ~", paste(n[!n %in% "Estimate"], collapse = " + ")))  ## neuralnet doesn't recognize y~, so I define a formula 
nn <- neuralnet(f,data=train_,hidden=c(10,7),linear.output=T)  ##usually 2/3 of the input size(25); two layers(10,6); 25:10:7:1. Also, linear,output=T means we want regression model
plot(nn)
###   The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model.
###   The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used.
```


```{r} 
### Predicting Estimate using the neural network

pr.nn <- compute(nn,test_[,c(1:17,19:26)])

pr.nn_ <- pr.nn$net.result*(max(data$Estimate)-min(data$Estimate))+min(data$Estimate)
test.r <- (test_$Estimate)*(max(data$Estimate)-min(data$Estimate))+min(data$Estimate)

MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)

print(paste(MSE.lm,MSE.nn))   ### compare MSE from linear model(left) and MSE from nerualnet(right)

## perform a fast cross validation in order to be more confident about the results.
par(mfrow=c(1,3))

plot(test$Estimate,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')

plot(test$Estimate,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)

plot(test$Estimate,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$Estimate,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
```

```{r}
library(boot)
set.seed(1)
lm.fit <- glm(Estimate~.,data=data)
cv.glm(data,lm.fit,K=10)$delta[1]



set.seed(450)
cv.error <- NULL
k <- 10

library(plyr) 
pbar <- create_progress_bar('text')
pbar$init(k)

for(i in 1:k){
    index <- sample(1:nrow(data),round(0.9*nrow(data)))
    train.cv <- scaled[index,]
    test.cv <- scaled[-index,]
    
    nn <- neuralnet(f,data=train.cv,hidden=c(10,7),linear.output=T)
    
    pr.nn <- compute(nn,test.cv[,1:17,19:26])
    pr.nn <- pr.nn$net.result*(max(data$Estimate)-min(data$Estimate))+min(data$Estimate)
    
    test.cv.r <- (test.cv$Estimate)*(max(data$Estimate)-min(data$Estimate))+min(data$Estimate)
    
    cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)
    
    pbar$step()
}



mean(cv.error)
boxplot(cv.error,xlab='MSE CV',col='cyan',
        border='blue',names='CV error (MSE)',
        main='CV error (MSE) for NN',horizontal=TRUE)
```


```


