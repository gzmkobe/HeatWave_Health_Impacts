%IncMSE is the most robust and informative measure. It is the increase in mse of predictions(estimated with out-of-bag-CV) as 
a result of variable j 
being permuted(values randomly shuffled).

grow regression forest. Compute OOB-mse, name this mse0.
for 1 to j var: permute values of column j, then predict and compute OOB-mse(j)
%IncMSE of j'th is (mse(j)-mse0)/mse0 * 100%
the higher number, the more important

IncNodePurity relates to the loss function which by best splits are chosen. The loss function is mse for regression and 
gini-impurity for classification. 
More useful variables achieve higher increases in node purities, that is to find a split which has a high inter node 'variance' 
and a small intra node 'variance'. 
IncNodePurity is biased and should only be used if the extra computation time of calculating %IncMSE is unacceptable. Since it 
only takes ~5-25% extra time to calculate %IncMSE, 
this would almost never happen.








The first one can be 'interpreted' as follows: if a predictor is important in your current model, then assigning other values for 
that predictor randomly 
but 'realistically' (i.e.: permuting this predictor's values over your dataset), should have a negative influence on prediction, 
i.e.: using the same model to predict 
from data that is the same except for the one variable, should give worse predictions.

So, you take a predictive measure (MSE) with the original dataset and then with the 'permuted' dataset, and you compare them 
somehow. One way, particularly 
since we expect the original MSE to always be smaller, the difference can be taken. Finally, for making the values comparable over 
variables, these are scaled.

For the second one: at each split, you can calculate how much this split reduces node impurity (for regression trees, indeed, the 
difference between RSS before 
and after the split). This is summed over all splits for that variable, over all trees.
