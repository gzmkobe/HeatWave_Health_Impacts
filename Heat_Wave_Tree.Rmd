---
title: "Untitled"
author: "Zhiming Guo"
date: "May 23, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
##load data
load("data-raw/ListOfAllHeatwaves.Rdata")
hw=data.frame(all.hws)
colnames(hw)[28]<="mean.temp.1"
##################################################
################################ Regression Tree
library(MASS)
library(tree)


### !!! data hw doesn't include post.estimate but has Estimate
hw=hw[,-c(1,6,7,20,22,31:35)] ##Since Regression Tree only allows maximum 32 factors, also, there are some useless columns 

########################### If reponse is Estimate
### Single Tree
set.seed(1) ##generate fixed random numbers
train = sample (1: nrow (hw), nrow(hw)/2)
tree.hw = tree(Estimate~.,hw, subset = train )
summary (tree.hw)
## Variables actually used in tree construction:"max.temp.quantile" "Ppoverty"          "mean.temp"
plot(tree.hw)
text(tree.hw,pretty=0)
 cv.hw =cv.tree(tree.hw)
plot(cv.hw$size,cv.hw$dev,type='b')
## The plot tells us the best would be 1 node, but it doesn;t make sense.


## disregard pruned tree, but using unpruned tree
yhat=predict(tree.hw,newdata=hw[-train,])
hw.test=hw[-train ,'Estimate']
plot(yhat,hw.test)
abline(0 ,1)
mean((yhat - hw.test)^2)
## MSE=0.03582249
```

```{r}
#### Bagging
library(randomForest)
set.seed(1)
bag.hw=randomForest(Estimate~.,data=hw,subset=train,mtry=24,importance=TRUE)
yhat.bag=predict(bag.hw,newdata=hw[-train ,])
plot(yhat.bag,hw.test)
abline(0,1)
mean((yhat.bag - hw.test)^2)
##  MSE=0.03629245 even bigger than single tree.
importance(bag.hw)
varImpPlot(bag.hw)
```

```{r}
### Tuning
fitControl=trainControl(method="cv",
                        number=10,)


            

```

```{r}
#### random forest
library(caret)

#### Tuning to find best mtry for Random Forest
set.seed(1)
tuning.rf.hw=train(Estimate~.,
                   data=hw,subset=train,
                   method = "rf",
                   trControl = fitControl,
                   ntree = 10,
                   importance = TRUE,
                   metric="RMSE",
                   maximize = FALSE,
                   tuneLength=5)
tuning.rf.hw
plot(tuning.rf.hw)
#### the optimal mtry is 2.



set.seed(1)
rf.hw=randomForest(Estimate~.,
                   data=hw,subset=train,
                   mtry=2,
                   importance=TRUE)
yhat.bag=predict(rf.hw,newdata=hw[-train ,])
plot(yhat.bag,hw.test)
abline(0,1)
mean((yhat.bag - hw.test)^2)
## MSE=0.0354102
importance(rf.hw)
varImpPlot(rf.hw)



```


```{r}
### boosting
library(gbm)

#### Tuning to find best mtry for Boostingt
gbmgrid=expand.grid(.interaction.depth = seq(1, 7, by = 2),
                    .n.trees = seq(200,300,by = 50),
                    .shrinkage =0.01,
                    .n.minobsinnode=10)
set.seed(1)
## I've already tested shrinkage=0.1, which is not good as shrinkage=0.01, to speed the process, I cut 0.1 off.



tuning.boost.hw=train(Estimate~.,data=hw[train,],
                      method="gbm",
                      tuneGrid=gbmgrid,
                      trControl=fitControl,
                      verbose=FALSE)
tuning.boost.hw
plot(tuning.boost.hw)
## The plot has the optimal value at interaction.depth=5 and n.trees=200. 



set.seed(1)
boost.hw=gbm(Estimate~.,data = hw[train,],distribution = "gaussian",n.trees=200,interaction.depth = 5)
summary(boost.hw)
par(mfrow =c(1,2) )
plot( boost.hw ,i="pop100")
plot( boost.hw,i="Ppoverty")
yhat.boost = predict(boost.hw, newdata = hw[-train ,],n.trees =200)
mean((yhat.boost - hw.test)^2)

## MSE=0.03561749


```

```{r}
#### If response is post.est
load("F:/Study/study/Summer Epidemiology Research/ListOfAllHeatwaves.Rdata")
hw=data.frame(all.hws)
colnames(hw)[28]<="mean.temp.1"

hw=hw[,-c(1,6,7,20:22,31:33,35)]
### Single Tree
set.seed(1) ##generate fixed random numbers
train = sample (1: nrow (hw), nrow(hw)/2)
tree.hw = tree(post.estimate~.,hw, subset = train )
summary (tree.hw)
## Variables actually used in tree construction:"max.temp.quantile" "Ppoverty"          "mean.temp"
plot(tree.hw)
text(tree.hw,pretty=0)
cv.hw =cv.tree(tree.hw)
plot(cv.hw$size,cv.hw$dev,type='b')
## The plot tells us the best would be 4 node.

## Continue using Prune
prune.hw =prune.tree(tree.hw,best=4)  ## error
plot(prune.hw)
text(prune.hw,pretty =0)

## disregard pruned tree, but using unpruned tree
yhat=predict(tree.hw,newdata=hw[-train,])
hw.test=hw[-train ,'post.estimate']
plot(yhat,hw.test)
abline(0 ,1)
mean((yhat - hw.test)^2)
## MSE=0.0009294525 Really lower!
```

```{r}
#### Bagging
library(randomForest)
set.seed(1)
bag.hw=randomForest(post.estimate~.,data=hw,subset=train,mtry=24,importance=TRUE)
yhat.bag=predict(bag.hw,newdata=hw[-train ,])
plot(yhat.bag,hw.test)
abline(0,1)
mean((yhat.bag - hw.test)^2)
## MSE=0.000927588 does't change a lot
importance(bag.hw)
varImpPlot(bag.hw)
```

```{r}
#### random forest
#### Tuning to find best mtry for Random Forest
set.seed(1)
tuning.rf.hw=train(post.estimate~.,
                   data=hw,subset=train,
                   method = "rf",
                   trControl = fitControl,
                   ntree = 10,
                   importance = TRUE,
                   metric="RMSE",
                   maximize = FALSE,
                   tuneLength=5)
tuning.rf.hw
plot(tuning.rf.hw)
#### the optimal mtry is 2.


set.seed(1)
rf.hw=randomForest(post.estimate~.,data=hw,subset=train,mtry=2,importance=TRUE)
yhat.bag=predict(rf.hw,newdata=hw[-train ,])
plot(yhat.bag,hw.test)
abline(0,1)
mean((yhat.bag - hw.test)^2)
## MSE=0.03599363 doesn't change a lot
importance(rf.hw)
varImpPlot(rf.hw)

```

```{r}
### boosting
#### Tuning to find best mtry for Boostingt
gbmgrid=expand.grid(.interaction.depth = 7,
                    .n.trees = c(200,250),
                    .shrinkage =0.01,
                    .n.minobsinnode=10)
### shrinkage=0.1 is too bad, the optimal depth is 7 after testing several combinations

set.seed(1)
tuning.boost.hw=train(post.estimate~.,data=hw[train,],
                      method="gbm",
                      tuneGrid=gbmgrid,
                      trControl=fitControl,
                      verbose=FALSE)
tuning.boost.hw
plot(tuning.boost.hw)


set.seed(1)
boost.hw=gbm(post.estimate~.,data = hw[train,],distribution = "gaussian",n.trees=250,interaction.depth = 7)
summary(boost.hw)
par(mfrow =c(1,2))
plot( boost.hw ,i="mean.temp.quantile")
plot( boost.hw,i="start.doy")
yhat.boost = predict(boost.hw, newdata = hw[-train ,],n.trees =250)
mean((yhat.boost - hw.test)^2)

## MSE=0.03650557


```
